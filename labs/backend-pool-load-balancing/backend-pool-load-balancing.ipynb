{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ❤️ OpenAI\n",
    "\n",
    "## バックエンドプール負荷分散 ラボ\n",
    "![flow](../../images/backend-pool-load-balancing.gif)\n",
    "\n",
    "APIMの[バックエンドプール機能](https://learn.microsoft.com/en-us/azure/api-management/backends?tabs=bicep)を使用して、Azure OpenAIエンドポイントまたはモックサーバーのリストに対して組み込みの負荷分散を試すためのプレイグラウンドです。\n",
    "\n",
    "注意事項:\n",
    "- バックエンドプールはデフォルトでラウンドロビンを使用します\n",
    "- しかし、優先度と重み付けに基づくルーティングもサポートされています: `openai_resources`変数の`priority`（数値が低いほど優先度が高い）および`weight`パラメータを調整してください\n",
    "- `retry` API Managementポリシーは、HTTP 429ステータスコードが発生した場合に利用可能なバックエンドへの再試行を開始します\n",
    "- `openai_resources`変数をクリアしてモックサーバーを使用し、[app.py](../../tools/mock-server/app.py)ファイルのコードを変更してカスタム動作をシミュレートします\n",
    "\n",
    "### 結果\n",
    "![result](result.png)\n",
    "\n",
    "### 目次\n",
    "- [0️⃣ ノートブック変数の初期化](#0)\n",
    "- [1️⃣ Azureリソースグループの作成](#1)\n",
    "- [2️⃣ 🦾 Bicepを使用したデプロイの作成](#2)\n",
    "- [3️⃣ デプロイ出力の取得](#3)\n",
    "- [🧪 直接HTTPコールを使用してAPIをテストする](#requests)\n",
    "- [🔍 負荷分散結果の分析](#plot)\n",
    "- [🧪 Azure OpenAI Python SDKを使用してAPIをテストする](#sdk)\n",
    "- [🗑️ リソースのクリーンアップ](#clean)\n",
    "\n",
    "### 前提条件\n",
    "- [Python 3.8以降のバージョン](https://www.python.org/)がインストールされていること\n",
    "- [Pandasライブラリ](https://pandas.pydata.org/)およびmatplotlibがインストールされていること\n",
    "- [VS Code](https://code.visualstudio.com/)がインストールされ、[Jupyterノートブック拡張機能](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter)が有効になっていること\n",
    "- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)がインストールされていること\n",
    "- Contributor権限を持つ[Azureサブスクリプション](https://azure.microsoft.com/en-us/free/)があること\n",
    "- [Azure OpenAIへのアクセス](https://aka.ms/oai/access)が許可されているか、モックサービスを有効にすること\n",
    "- [Azure CLIでAzureにサインイン](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-interactively)していること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mA web browser has been opened at https://login.microsoftonline.com/fdb5bdfe-92ed-4d5a-b697-a66439451bda/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\u001b[0m\n",
      "\n",
      "Retrieving subscriptions for the selection...\n",
      "\n",
      "[Tenant and subscription selection]\n",
      "\n",
      "No     Subscription name    Subscription ID                       Tenant\n",
      "-----  -------------------  ------------------------------------  ------------------------------------\n",
      "\u001b[96m[1]\u001b[0m *  \u001b[96mkoheisaito\u001b[0m           \u001b[96m59e7c4a1-e556-4873-87e6-ba29af832e01\u001b[0m  \u001b[96mfdb5bdfe-92ed-4d5a-b697-a66439451bda\u001b[0m\n",
      "\n",
      "The default is marked with an *; the default tenant is 'fdb5bdfe-92ed-4d5a-b697-a66439451bda' and subscription is 'koheisaito' (59e7c4a1-e556-4873-87e6-ba29af832e01).\n",
      "\n",
      "Select a subscription and tenant (Type a number or Enter for no changes): ^C\n"
     ]
    }
   ],
   "source": [
    "! az login --tenant \"fdb5bdfe-92ed-4d5a-b697-a66439451bda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0️⃣ ノートブック変数の初期化\n",
    "\n",
    "- リソースはサブスクリプションIDに基づく一意の文字列でサフィックスされます\n",
    "- ```mock_webapps``` 変数は、モック機能のためにデプロイされたWebアプリのリストを設定します。モックサービスでOpenAIの動作をシミュレートするために ```openai_resources``` リストをクリアしてください。\n",
    "- ロケーションパラメータは、[Azureリージョンごとの製品の利用可能性](https://azure.microsoft.com/ja-jp/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management)に基づいて調整してください。\n",
    "- OpenAIモデルとバージョンは、[リージョンごとの利用可能性](https://learn.microsoft.com/ja-jp/azure/ai-services/openai/concepts/models)に基づいて調整してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "resource_group_name = f\"lab-{deployment_name}\" # change the name to match your naming style\n",
    "resource_group_location = \"westeurope\"\n",
    "apim_resource_name = \"apim\"\n",
    "apim_resource_location = \"westeurope\"\n",
    "apim_resource_sku = \"Basicv2\"\n",
    "openai_resources = [ {\"name\": \"openai1\", \"location\": \"uksouth\", \"priority\": 1, \"weight\": 80}, {\"name\": \"openai2\", \"location\": \"swedencentral\", \"priority\": 1, \"weight\": 10}, {\"name\": \"openai3\", \"location\": \"francecentral\", \"priority\": 1, \"weight\": 10} ] # list of OpenAI resources to deploy. Clear this list to use only the mock resources\n",
    "openai_resources_sku = \"S0\"\n",
    "openai_model_name = \"gpt-35-turbo\"\n",
    "openai_model_version = \"0613\"\n",
    "openai_deployment_name = \"gpt-35-turbo\"\n",
    "openai_api_version = \"2024-02-01\"\n",
    "openai_specification_url='https://raw.githubusercontent.com/Azure/azure-rest-api-specs/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/stable/' + openai_api_version + '/inference.json'\n",
    "openai_backend_pool = \"openai-backend-pool\"\n",
    "mock_backend_pool = \"mock-backend-pool\"\n",
    "mock_webapps = [ {\"name\": \"openaimock1\", \"endpoint\": \"https://openaimock1.azurewebsites.net\", \"priority\": 1, \"weight\": 80}, {\"name\": \"openaimock2\", \"endpoint\": \"https://openaimock2.azurewebsites.net\", \"priority\": 1, \"weight\": 20} ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1️⃣ Azureリソースグループの作成\n",
    "このラボでデプロイされるすべてのリソースは、指定されたリソースグループに作成されます。既存のリソースグループを使用する場合は、このステップをスキップしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Azure Resource Group  lab-backend-pool-load-balancing  created ⌚  14:14:20.836640\n"
     ]
    }
   ],
   "source": [
    "resource_group_stdout = ! az group create --name {resource_group_name} --location {resource_group_location}\n",
    "if resource_group_stdout.n.startswith(\"ERROR\"):\n",
    "    print(resource_group_stdout)\n",
    "else:\n",
    "    print(\"✅ Azure Resource Group \", resource_group_name, \" created ⌚ \", datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2️⃣ 🦾 Bicepを使用したデプロイの作成\n",
    "\n",
    "このラボでは、[Bicep](https://learn.microsoft.com/ja-jp/azure/azure-resource-manager/bicep/overview?tabs=bicep)を使用して、デプロイされるすべてのリソースを宣言的に定義します。\n",
    "\n",
    "`openAIModelCapacity`は意図的に低く設定されており（2kトークン/分）、ロードバランサーのリトライロジックを示すために設定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(openai_resources) > 0:\n",
    "    backend_id = openai_backend_pool if len(openai_resources) > 1 else openai_resources[0].get(\"name\")\n",
    "elif len(mock_webapps) > 0:\n",
    "    backend_id = mock_backend_pool if len(mock_webapps) > 1 else mock_webapps[0].get(\"name\")\n",
    "\n",
    "with open(\"policy.xml\", 'r') as policy_xml_file:\n",
    "    policy_template_xml = policy_xml_file.read()\n",
    "    policy_xml = policy_template_xml.replace(\"{backend-id}\", backend_id)\n",
    "    policy_xml_file.close()\n",
    "open(\"policy.xml\", 'w').write(policy_xml)\n",
    "\n",
    "bicep_parameters = {\n",
    "  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n",
    "  \"contentVersion\": \"1.0.0.0\",\n",
    "  \"parameters\": {\n",
    "    \"mockWebApps\": { \"value\": mock_webapps },\n",
    "    \"mockBackendPoolName\": { \"value\": mock_backend_pool },\n",
    "    \"openAIBackendPoolName\": { \"value\": openai_backend_pool },\n",
    "    \"openAIConfig\": { \"value\": openai_resources },\n",
    "    \"openAIDeploymentName\": { \"value\": openai_deployment_name },\n",
    "    \"openAISku\": { \"value\": openai_resources_sku },\n",
    "    \"openAIModelName\": { \"value\": openai_model_name },\n",
    "    \"openAIModelVersion\": { \"value\": openai_model_version },\n",
    "    \"openAIModelCapacity\": { \"value\": 2 },\n",
    "    \"openAIAPISpecURL\": { \"value\": openai_specification_url },\n",
    "    \"apimResourceName\": { \"value\": apim_resource_name},\n",
    "    \"apimResourceLocation\": { \"value\": apim_resource_location},\n",
    "    \"apimSku\": { \"value\": apim_resource_sku}\n",
    "  }\n",
    "}\n",
    "with open('params.json', 'w') as bicep_parameters_file:\n",
    "    bicep_parameters_file.write(json.dumps(bicep_parameters))\n",
    "\n",
    "! az deployment group create --name {deployment_name} --resource-group {resource_group_name} --template-file \"main.bicep\" --parameters \"params.json\"\n",
    "\n",
    "open(\"policy.xml\", 'w').write(policy_template_xml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3️⃣ Get the deployment outputs\n",
    "\n",
    "We are now at the stage where we only need to retrieve the gateway URL and the subscription before we are ready for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_stdout = ! az deployment group show --name {deployment_name} -g {resource_group_name} --query properties.outputs.apimSubscriptionKey.value -o tsv\n",
    "apim_subscription_key = deployment_stdout.n\n",
    "deployment_stdout = ! az deployment group show --name {deployment_name} -g {resource_group_name} --query properties.outputs.apimResourceGatewayURL.value -o tsv\n",
    "apim_resource_gateway_url = deployment_stdout.n\n",
    "print(\"👉🏻 API Gateway URL: \", apim_resource_gateway_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### 🧪 Test the API using a direct HTTP call\n",
    "Requests is an elegant and simple HTTP library for Python that will be used here to make raw API requests and inspect the responses. \n",
    "\n",
    "You will not see HTTP 429s returned as API Management's `retry` policy will select an available backend. If no backends are viable, an HTTP 503 will be returned.\n",
    "\n",
    "Tip: Use the [tracing tool](../../tools/tracing.ipynb) to track the behavior of the backend pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "runs = 20\n",
    "sleep_time_ms = 0\n",
    "url = apim_resource_gateway_url + \"/openai/deployments/\" + openai_deployment_name + \"/chat/completions?api-version=\" + openai_api_version\n",
    "api_runs = []\n",
    "\n",
    "for i in range(runs):\n",
    "    print(\"▶️ Run:\", i+1, \"/\", runs)\n",
    "    \n",
    "    if len(openai_resources) > 0:\n",
    "        messages={\"messages\":[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ]}\n",
    "    elif len(mock_webapps) > 0:\n",
    "        messages={\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": {\n",
    "                        \"simulation\": {\n",
    "                            \"default\": {\"response_status_code\": 200, \"wait_time_ms\": 0},\n",
    "                            \"openaimock1.azurewebsites.net\": {\"response_status_code\": 429}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers = {'api-key':apim_subscription_key}, json = messages)\n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"⌚ {response_time:.2f} seconds\")\n",
    "    # Check the response status code and apply formatting\n",
    "    if 200 <= response.status_code < 300:\n",
    "        status_code_str = '\\x1b[1;32m' + str(response.status_code) + \" - \" + response.reason + '\\x1b[0m'  # Bold and green\n",
    "    elif response.status_code >= 400:\n",
    "        status_code_str = '\\x1b[1;31m' + str(response.status_code) + \" - \" + response.reason + '\\x1b[0m'  # Bold and red\n",
    "    else:\n",
    "        status_code_str = str(response.status_code)  # No formatting\n",
    "\n",
    "    # Print the response status with the appropriate formatting\n",
    "    print(\"Response status:\", status_code_str)\n",
    "    \n",
    "    print(\"Response headers:\", response.headers)\n",
    "    \n",
    "    if \"x-ms-region\" in response.headers:\n",
    "        print(\"x-ms-region:\", '\\x1b[1;31m'+response.headers.get(\"x-ms-region\")+'\\x1b[0m') # this header is useful to determine the region of the backend that served the request\n",
    "        api_runs.append((response_time, response.headers.get(\"x-ms-region\")))\n",
    "    \n",
    "    if (response.status_code == 200):\n",
    "        data = json.loads(response.text)\n",
    "        print(\"Token usage:\", data.get(\"usage\"), \"\\n\")\n",
    "        print(\"💬 \", data.get(\"choices\")[0].get(\"message\").get(\"content\"), \"\\n\")\n",
    "    else:\n",
    "        print(response.text)   \n",
    "\n",
    "    time.sleep(sleep_time_ms/1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### 🔍 Analyze Load Balancing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [15, 7]\n",
    "df = pd.DataFrame(api_runs, columns=['Response Time', 'Region'])\n",
    "df['Run'] = range(1, len(df) + 1)\n",
    "\n",
    "# Define a color map for each region\n",
    "color_map = {'UK South': 'lightpink', 'France Central': 'lightblue', 'Sweden Central': 'lightyellow', 'Region3': 'red', 'Region4': 'orange'}  # Add more regions and colors as needed\n",
    "\n",
    "# Plot the dataframe with colored bars\n",
    "ax = df.plot(kind='bar', x='Run', y='Response Time', color=[color_map.get(region, 'gray') for region in df['Region']], legend=False)\n",
    "\n",
    "# Add legend\n",
    "legend_labels = [plt.Rectangle((0, 0), 1, 1, color=color_map.get(region, 'gray')) for region in df['Region'].unique()]\n",
    "ax.legend(legend_labels, df['Region'].unique())\n",
    "\n",
    "plt.title('Load Balancing results')\n",
    "plt.xlabel('Runs')\n",
    "plt.ylabel('Response Time')\n",
    "plt.xticks(df['Run'], rotation=0)\n",
    "\n",
    "average = df['Response Time'].mean()\n",
    "plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### 🧪 Test the API using the Azure OpenAI Python SDK\n",
    "\n",
    "Repeat the same test using the Python SDK to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "runs = 9\n",
    "sleep_time_ms = 0\n",
    "\n",
    "for i in range(runs):\n",
    "    print(\"▶️ Run: \", i+1)\n",
    "\n",
    "    if len(openai_resources) > 0:\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ]\n",
    "    elif len(mock_webapps) > 0:\n",
    "        messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": {\n",
    "                        \"simulation\": {\n",
    "                            \"default\": {\"response_status_code\": 200, \"wait_time_ms\": 0},\n",
    "                            \"openaimock1.azurewebsites.net\": {\"response_status_code\": 429}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=apim_resource_gateway_url,\n",
    "        api_key=apim_subscription_key,\n",
    "        api_version=openai_api_version\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model=openai_model_name, messages=messages)\n",
    "    response_time = time.time() - start_time\n",
    "    print(f\"⌚ {response_time:.2f} seconds\")\n",
    "    print(\"💬 \", response.choices[0].message.content)\n",
    "    time.sleep(sleep_time_ms/1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### 🗑️ Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
